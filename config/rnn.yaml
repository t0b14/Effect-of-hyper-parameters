experiment:
  title: Single-Layer RNN
  options:
    saving_tag: "last"
    run_name: "final_run"
    train_n_test: 1
    visualize: 1
    use_wandb: 1
    make_weights_histograms: 1
    make_gradients_plot: 1
    make_hidden_state_plot: 1
    length_skipped_in_data: 400
    show_plot: 0

  model:
    network_name: rnn
    in_dim: 4
    hidden_dims: 100 #100
    out_dim: 1
    tau: 10.
    num_processes: 1
    hidden_noise: 0.1
    activation_func: "tanh" #"tanh" # tanh, relu ( all lower case )
  
  training:
    dataset_name: ctxt
    n_epochs: 300
    batch_size: 16
    total_seq_length: 1000
    seq_length: 100 # larger lengths won't fit well into cpu memory; DO NOT CHANGE
    training_help: 0 # 0 to n_epochs (iterations)
    hidden_dims: 100
    n_trials: 20 # never less than 10 for operative dimension
    with_inputnoise: 1
    noise_level: 10 # number from 1 to 10; requires with_inputnoise
    coherency_intervals: "uniform" #"original" #"uniform" #  or "uniform"
    output_path: ./io/output/rnn1
    output_path_plots: ./io/output/rnn1/histograms_and_plots

  optimizer:
    optimizer_name: adagrad
    lr: 0.01
    betas: [0.9, 0.999]
    eps: 1.e-7
    lr_decay: 0 
    weight_decay: 0 
    amsgrad: false
    momentum: 0.9
    apply_gradient_clipping: 1

  seed: 42