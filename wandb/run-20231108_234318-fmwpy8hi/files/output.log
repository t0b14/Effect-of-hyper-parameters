
  0%|                                                                                                                                                                                              | 0/1000 [00:00<?, ?it/s]














































































































Epoch[115/1000], Loss: 3284.6563:  12%|████████████████▊                                                                                                                                 | 115/1000 [03:47<29:13,  1.98s/it]
Traceback (most recent call last):
  File "C:\Users\tobia\Documents\GitHub\RNN_Tobia_Simmler\main.py", line 15, in <module>
    run(config["experiment"])
  File "C:\Users\tobia\Documents\GitHub\RNN_Tobia_Simmler\src\runner.py", line 62, in run
    tm.fit(num_epochs=config["training"]["n_epochs"])
  File "C:\Users\tobia\Documents\GitHub\RNN_Tobia_Simmler\src\training\training_abstractbaseclass.py", line 113, in fit
    self.train(gradients, seq_l, self.n_intervals,self.device, self.train_dataloader, self.step)
  File "C:\Users\tobia\Documents\GitHub\RNN_Tobia_Simmler\src\training\training_abstractbaseclass.py", line 148, in train
    out, loss, h_1 = f(partial_in, partial_tar, h_1, gradients=gradients)
  File "C:\Users\tobia\Documents\GitHub\RNN_Tobia_Simmler\src\training\training_abstractbaseclass.py", line 209, in step
    loss.backward()
  File "C:\Users\tobia\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "C:\Users\tobia\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\autograd\__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt